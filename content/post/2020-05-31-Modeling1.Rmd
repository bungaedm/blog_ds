---
title: "모델링 공부 1"
author: "SonJiwoo"
date: 2020-05-31T20:13:14-05:00
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

# Decision Tree
의사결정나무

```{r}
#rpart 사용
```
# Variance 낮추기
## Bagging
Bootstrap Aggregating의 약자이다.
Bagging을 이해하기 위해서는 Boostrap이 무엇인지 알아야한다. Bootstrap은 쉽게 말해 복원추출이라고 생각하면 된다.
일반적으로는 기존의 데이터셋의 크기와 같은 크기로 복원추출하여 여러 개의 Bootstrap Sample set을 만든다. 여기서 각각의 데이터셋으로 의사결정나무를 만들고, 이들을 통합하여 모형을 만든다. 그렇기 때문에 이를 Ensemble Learning이라고 하는 것이다.

## Random Forest
Random Forest는 Bagging의 업그레이드 버전이라고 생각하면 된다. Bagging이 행렬 데이터에서 행만을 재구성했다면, Random Forest는 행뿐만 아니라 열까지도 재구성하는 작업이다. 만약 Bagging에 있어서 엄청나게 영향력 강한 변수가 있다면, 아무리 부트스트랩 데이터셋이 다양하게 만들어졌다고 해도 결국 그 변수로 수렴하여 설명하는 의사결정나무 모형만이 나올 것이다. 이러한 문제점을 극복하기 위해 열도 재구성하는 것이다. 그리고 그러한 과정이 tree를 random하게 산출하기 때문에 Random Forest이라는 이름이 만들어졌다고 한다.

#Bias 낮추기
## AdaBoost
Boosting의 시초는 Adaboost이다. Adaboost는 약간 변태적인 성격을 가진 머신러닝기법이다. 일종의 오답풀이를 엄청하는 모델이라고 생각하면 된다. Bagging과 RandomForest가 다양한 Bootstrap sample set에 대해서 평행하게(parallel) 의사결정나무를 만드는 데에 반해, Boosting은 오분류된 데이터들에 대해서 가중치를 높여 다음에 더 높은 확률로 뽑은 후 모델을 만드는 것이 목적이다. 즉, Boosting은 Sequential하다.

## Gradient Boosting
Gradient Boosting도 Boosting의 일종이지만, Adaboost와 차이점이 하나 있다. Adaboost는 지속적으로 오분류된 데이터를 지속적으로 뽑는 데에 반해, Gradient boosting은 아예 error 자체를 예측하는 모형을 만든다. 이것을 계속 sequential하게 쫓아갈 경우, 기존의 big error가 small error가 되어 bias가 상당히 낮은 모형을 만들어 낼 수 있다.